\section{Bandit Approach Introduction}\label{sec:bandit_intro}

In the first half of this chapter, we explored the potential of a method that
combines Bayesian optimization with learning from preferences. However, the
simulation results presented demonstrate it is unlikely the proposed method
can scale to the dimensionality needed for prosthesis optimization. Previously
published impedance control strategies for transfemoral prostheses have roughly
20 tunable parameters for a given gait condition, such as walking at a specific
speed. \citet{sup2011upslope} show that these parameters also vary with
alternative conditions such as incline.  Therefore, these kinds of parameterized
policies could require on the order of 100 parameters to deal with a range of
situations. Previous work has attempted to reduce the number of parameters via
heuristic rules that tie impedance parameters to other states of the prosthesis
such as joint angles \citep{simon2014configuring}.  However, it is not obvious
how to translate these heuristics to other control strategies, such as the
neuromuscular control we propose in \cref{sec:nm_control_prosthesis}, or
phase-based control \citep{quintero2016preliminary}, which follows knee and
ankle trajectories parameterized as functions of hip angle and hip angle
integral.

To deal with high-dimensional parameter selection for prostheses, many have
turned to offline optimization of control parameters \citet{markowitz2011speed}
use data from a height-and weight-matched intact subject to obtain
speed-adaptive neuromuscular control parameters for a transtibial amputee's
prosthesis and \citet{aghasadeghi2013learning} use an invariant gait
representation to model an amputee's gait and find the appropriate impedance
control parameters. In these approaches however, it is unclear how well the
resultant parameters suit the subject when executed on actual hardware.

In this paper, we tackle these issues by framing prosthesis optimization as a
dueling bandits problem \citep{yue2012k}. The resulting approach utilizes the
subject's preferences to include subjective user feedback in the tuning process.
The method deals with high dimensional optimization problems by incorporating
domain knowledge in the form of an offline optimization step. We show that this
method produces a library of parameters from which different users prefer
different options and for which preferred controllers tend to follow human gait
trends. Moreover, we explore further utilizing the offline optimization to help
the controllers generalize to speeds that were not included during the online
optimization process.
